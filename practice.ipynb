{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06b9f18c",
   "metadata": {},
   "source": [
    "An ensemble method = combining multiple models to get higher accuracy, stability, and better generalization than a single model.\n",
    "\n",
    "Types of Ensemble Methods\n",
    "\n",
    "Bagging (Bootstrap Aggregating)\n",
    "\n",
    "Many models trained in parallel on random subsets.\n",
    "\n",
    "Example: Random Forest\n",
    "\n",
    "Boosting\n",
    "\n",
    "Models trained sequentially, each fixing the previous one’s mistakes.\n",
    "\n",
    "Examples: AdaBoost, Gradient Boosting, XGBoost\n",
    "\n",
    "Stacking\n",
    "\n",
    "Different models (like decision trees, SVM, logistic regression) are combined, and their outputs are fed into a meta-model for final prediction.\n",
    "\n",
    "A decision stump is just a very simple decision tree with only one split (one level)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ea1ade",
   "metadata": {},
   "source": [
    "AdaBoost\n",
    "\n",
    "Error Handling: Reweights misclassified samples.\n",
    "Loss Function: No explicit loss; reduces classification error.\n",
    "✅ Best for small datasets, interpretability, and imbalanced data.\n",
    "❌ Sensitive to noise, weaker on complex data.\n",
    "\n",
    "Gradient Boosting (GBM)\n",
    "\n",
    "Error Handling: Fits to residuals using gradient descent.\n",
    "Loss Function: Explicit, differentiable loss functions.\n",
    "✅ Best for general regression/classification tasks.\n",
    "❌ Slower training compared to AdaBoost/XGBoost.\n",
    "\n",
    "XGBoost\n",
    "\n",
    "Error Handling: Same as GBM but optimized.\n",
    "Regularization: Built-in (L1, L2) → prevents overfitting.\n",
    "Optimization: Parallelization + scalability.\n",
    "✅ Best for large datasets, high performance, competitions.\n",
    "❌ More complex to tune."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf57785",
   "metadata": {},
   "source": [
    "XGBoost achieves its speed and efficiency through a combination of algorithmic and engineering optimizations:\n",
    "Parallel Processing:\n",
    "Handling Missing Data:\n",
    "Tree Pruning (Depth-wise growth with post-pruning):\n",
    "Column Block (Cache-aware) and Parallel Learning:\n",
    "Regularization in XGBoost:\n",
    "Out-of-Core Computing:\n",
    "(For datasets larger than available memory, XGBoost can use disk-based data structures, enabling it to handle massive datasets efficiently)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33191e94",
   "metadata": {},
   "source": [
    "Why Cross-Validation is Important in Boosting?\n",
    "\n",
    "Prevents Overfitting\n",
    "Helps in Hyperparameter Tuning\n",
    "\n",
    "Parameters like:\n",
    "Number of estimators (trees)\n",
    "Learning rate\n",
    "Tree depth (in case of stumps, max_depth=1)\n",
    "Using k-fold CV lets us select the best combination that balances bias vs variance.\n",
    "\n",
    "Avoids Bias from One Train-Test Split\n",
    "A single train-test split might give lucky/unlucky results.\n",
    "CV averages across folds → more reliable performance estimate.\n",
    "Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d00957",
   "metadata": {},
   "source": [
    "Key Hyperparameters in AdaBoost, Gradient Boosting, XGBoost; Interaction of Learning Rate & Number of Estimators\n",
    "AdaBoost\n",
    "\n",
    "n_estimators: number of weak learners.\n",
    "earning_rate: shrinkage per learner.\n",
    "\n",
    "Base learner parameters (e.g., tree depth).\n",
    "Gradient Boosting (GBM)\n",
    "n_estimators\n",
    "learning_rate\n",
    "max_depth\n",
    "min_samples_split, min_samples_leaf\n",
    "Subsampling parameters—e.g., row and column sampling.\n",
    "\n",
    "XGBoost\n",
    "n_estimators\n",
    "learning_rate (eta)\n",
    "max_depth\n",
    "alpha (L1 regularization), lambda (L2)\n",
    "min_child_weight, gamma, subsample, colsample_bytree\n",
    "\n",
    "\n",
    "Over/Underfitting & Curves\n",
    "Learning/validation curves help visualize model behavior and guide tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cf304a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
